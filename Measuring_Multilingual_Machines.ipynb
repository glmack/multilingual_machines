{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Multilingual Machines\n",
    "#### BLEU Scores and Cross-Lingual Machine Learning\n",
    "\n",
    "#### by Lee Mackey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies an article of the same name published on Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your machine learn in Chinese? 您和您的機器學習中文嗎? I don’t speak a word of Mandarin or Cantonese so Google Translate gets all the credit — good or bad — for the preceding sentence. But how could you quickly evaluate the quality of this machine translation? This challenge encapsulates the basic demand that gives rise to the BLEU metric. BLEU, which stands for bilingual language understudy, is the default measure of machine translation quality and is also sometimes applied to more general cross-lingual approaches to natural language processing (NLP). The metric is well-established in the machine translation space but some analysts also question the application to a wider set of NLP tasks beyond the original purpose for which the algorithm was developed. This article explores these issues by briefly discussing basic lessons and limits of BLEU using examples drawn from the multilingual space of global patent documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "from nltk.translate.bleu_score import (sentence_bleu, corpus_bleu, \n",
    "                                       modified_precision, \n",
    "                                       SmoothingFunction)\n",
    "\n",
    "from nltk import (bigrams, trigrams, ngrams, sent_tokenize, word_tokenize)\n",
    "\n",
    "import tokenize\n",
    "import textwrap\n",
    "import string\n",
    "\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers at IBM developed the BLEU algorithm in 2002 as an efficient method to evaluate machine translation tasks that would otherwise require human evaluators. The original paper by the developers, Papineni and colleagues, is a good place to start if you’re interested in the founding milieu and details of the algorithm [1]. BLEU is an adjusted measure of precision of the overlap of word sequences between a “candidate” machine translation and one or more “reference” human translations. Conceptualizing the algorithm at the unit of a machine-translated sentence, BLEU counts the maximum number of times that word sequences, expressed by the term n-grams, occur in human-translated sentences. The adjusted counts of each n-gram in the sentence are summed and the number is then “adjusted” by dividing by the total (unclipped) number of n-grams in the candidate text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU score is as a number between 0 and 1, where 0 represents the complete absence of overlap in n-grams between candidate and reference texts, and where 1 might equal a machine translation that is exactly similar to one of the reference texts. While this example considers BLEU at the level of the sentence, the actual evaluation of a machine translation is calculated by averaging out sentence scores across an entire corpus and adjusting this aggregate metric to account for the typically-longer word lengths of machine translations. If you’re interested in learning more, you might learn to calculate BLEU in a fifteen-minute video by Andrew Ng of DeepLearning.Ai. To make learning BLEU more tangible, I begin with machine translations and human translations of Chinese text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of BLEU using Patent Texts as Examples\n",
    "A growing share of patents in the machine learning space are originally written and filed in Chinese, according to a recent report by the global governance organization of patents. To explore the basics of BLEU in more tangible manner, we first begin with the international filing of a Chinese language patent by the e-commerce company Alibaba for an invention related to natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fc871b490377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'patent_examples.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "with open('patent_examples.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split()\n",
    "        d[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4a5cde44319d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'patent_examples.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "examples = {}\n",
    "with open('patent_examples.txt', \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\"\\t\")\n",
    "        examples[key] = value\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 2 column 51 (char 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-359588421abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'patent_examples.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid control character at: line 2 column 51 (char 52)"
     ]
    }
   ],
   "source": [
    "with open('patent_examples.txt', 'r') as f:\n",
    "   f = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x = pd.read_csv('patent_examples.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title_cn   机器处理及文本纠错方法和装置、计算设备以及存储介质</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original_summary_cn 本发明公开了一种机器处理及文本纠错方法和装置、计算设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>，具体包括错误文本和对应的正确文本的纠错改写对, 以纠错改写对作为训练语料，对机器处理模型</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>进行训练，由此准备好适用于文本纠错的机器处理模型。可以通过从日志中挖掘纠错改写对来对机器</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>处理模型进行训练，使其适于对文本进行纠错。将第一文本输入到机器处理模型中，得到第二文本，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>即纠错结果文本。另外，还可以使用语言模型或常用词库先判断第一文本是否需要进行纠错。可以使</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>用从日志中挖掘出的训练语料来训练语言模型，也可以通过对日志中的文本进行分词、统计来整理常</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>用词库。由此，使得能够方便地实现文本纠错</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reference_human1_sentence   The training corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>to train the language model, or the common lex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>segmenting and counting text in the log.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reference_human1_summary    The invention disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text error correction method and device, a com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>storage medium, specifically comprising correc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pairs of incorrect  text and corresponding cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>and rewritten text pairs serving as a training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>machine processing model, thereby preparing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>suitable for text error correction. Through ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rewritten text pairs from a log, the machine p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>trained and thus made fit for text correction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>into the machine processing model to get the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>error correction result text. In addition, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>common lexicon can be used to determine whethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>be corrected. The training corpus extracted fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>train the language model, or the common lexico...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>segmenting and counting text in the log. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text error correction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>reference_human2_sentence   It can use the pra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>from the diary or daily journal to train the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>also initialize the common vocabulary bank thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>analysis of the diary or daily journal text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>segmenting and counting the text in the log.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>candidate_google_summary    The invention disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>text error correction method and device, a com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>storage medium, and particularly comprises an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>rewriting pair of an error text and a correspo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>an error correction rewriting pair as a traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>processing model. Training is performed, there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>processing model suitable for text correction....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>model can be trained to mine the error correct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>correction rewrite pair from the log. The firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>machine processing model to obtain a second te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>correction result text. In addition, you can u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>common lexicon to determine whether the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The language model can be trained using the tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>from the log, or the common lexicon can be org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>counting the text in the log. Thereby, text co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>candidate_wipo_sentence The training corpus ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>used to train the language model and also, thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>statistical analysis of text in the log compil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>used words.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>candidate_wipo_summary  The present invention ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>computing equipment and a storage medium. Spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>text and corresponding correct text, the corre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>a machine processing model, and in this way de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Through extraction of corrected and rewritten ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>and thus made fit for text correction by input...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>text i.e. a corrected text result. Moreover, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>assess whether text needs correction. The trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>model and also, through text segmentation and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>used words. Thus, text correction can be made ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        original_title_cn   机器处理及文本纠错方法和装置、计算设备以及存储介质\n",
       "0   original_summary_cn 本发明公开了一种机器处理及文本纠错方法和装置、计算设...\n",
       "1       ，具体包括错误文本和对应的正确文本的纠错改写对, 以纠错改写对作为训练语料，对机器处理模型\n",
       "2        进行训练，由此准备好适用于文本纠错的机器处理模型。可以通过从日志中挖掘纠错改写对来对机器\n",
       "3        处理模型进行训练，使其适于对文本进行纠错。将第一文本输入到机器处理模型中，得到第二文本，\n",
       "4        即纠错结果文本。另外，还可以使用语言模型或常用词库先判断第一文本是否需要进行纠错。可以使\n",
       "5        用从日志中挖掘出的训练语料来训练语言模型，也可以通过对日志中的文本进行分词、统计来整理常\n",
       "6                                用词库。由此，使得能够方便地实现文本纠错\n",
       "7   reference_human1_sentence   The training corpu...\n",
       "8   to train the language model, or the common lex...\n",
       "9            segmenting and counting text in the log.\n",
       "10  reference_human1_summary    The invention disc...\n",
       "11  text error correction method and device, a com...\n",
       "12  storage medium, specifically comprising correc...\n",
       "13  pairs of incorrect  text and corresponding cor...\n",
       "14  and rewritten text pairs serving as a training...\n",
       "15  machine processing model, thereby preparing a ...\n",
       "16  suitable for text error correction. Through ex...\n",
       "17  rewritten text pairs from a log, the machine p...\n",
       "18  trained and thus made fit for text correction ...\n",
       "19  into the machine processing model to get the s...\n",
       "20  error correction result text. In addition, the...\n",
       "21  common lexicon can be used to determine whethe...\n",
       "22  be corrected. The training corpus extracted fr...\n",
       "23  train the language model, or the common lexico...\n",
       "24  segmenting and counting text in the log. This ...\n",
       "25                             text error correction.\n",
       "26  reference_human2_sentence   It can use the pra...\n",
       "27  from the diary or daily journal to train the l...\n",
       "28  also initialize the common vocabulary bank thr...\n",
       "29        analysis of the diary or daily journal text\n",
       "..                                                ...\n",
       "47       segmenting and counting the text in the log.\n",
       "48  candidate_google_summary    The invention disc...\n",
       "49  text error correction method and device, a com...\n",
       "50  storage medium, and particularly comprises an ...\n",
       "51  rewriting pair of an error text and a correspo...\n",
       "52  an error correction rewriting pair as a traini...\n",
       "53  processing model. Training is performed, there...\n",
       "54  processing model suitable for text correction....\n",
       "55  model can be trained to mine the error correct...\n",
       "56  correction rewrite pair from the log. The firs...\n",
       "57  machine processing model to obtain a second te...\n",
       "58  correction result text. In addition, you can u...\n",
       "59  common lexicon to determine whether the first ...\n",
       "60  The language model can be trained using the tr...\n",
       "61  from the log, or the common lexicon can be org...\n",
       "62  counting the text in the log. Thereby, text co...\n",
       "63  candidate_wipo_sentence The training corpus ex...\n",
       "64  used to train the language model and also, thr...\n",
       "65  statistical analysis of text in the log compil...\n",
       "66                                        used words.\n",
       "67  candidate_wipo_summary  The present invention ...\n",
       "68  computing equipment and a storage medium. Spec...\n",
       "69  text and corresponding correct text, the corre...\n",
       "70  a machine processing model, and in this way de...\n",
       "71  Through extraction of corrected and rewritten ...\n",
       "72  and thus made fit for text correction by input...\n",
       "73  text i.e. a corrected text result. Moreover, a...\n",
       "74  assess whether text needs correction. The trai...\n",
       "75  model and also, through text segmentation and ...\n",
       "76  used words. Thus, text correction can be made ...\n",
       "\n",
       "[77 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e9c20f2d32b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     data = [(col1, col2)\n\u001b[0;32m----> 6\u001b[0;31m                 for col1, col2 in reader]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-e9c20f2d32b0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     data = [(col1, col2)\n\u001b[0;32m----> 6\u001b[0;31m                 for col1, col2 in reader]\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('patent_examples.txt') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    data = [(col1, col2)\n",
    "                for col1, col2 in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect example data: international patent for NLP invention by Alibaba\n",
    "\n",
    "# inspect title of patent in original Chinese\n",
    "original_title_cn = (\"\"\"机器处理及文本纠错方法和装置、计算设备以及存储介质\"\"\")\n",
    "\n",
    "# inspect summary of patent in original Chinese\n",
    "original_summary_cn = (\"\"\"本发明公开了一种机器处理及文本纠错方法和装置、计算设备以及存储介质\n",
    "，具体包括错误文本和对应的正确文本的纠错改写对, 以纠错改写对作为训练语料，对机器处理模型\n",
    "进行训练，由此准备好适用于文本纠错的机器处理模型。可以通过从日志中挖掘纠错改写对来对机器\n",
    "处理模型进行训练，使其适于对文本进行纠错。将第一文本输入到机器处理模型中，得到第二文本，\n",
    "即纠错结果文本。另外，还可以使用语言模型或常用词库先判断第一文本是否需要进行纠错。可以使\n",
    "用从日志中挖掘出的训练语料来训练语言模型，也可以通过对日志中的文本进行分词、统计来整理常\n",
    "用词库。由此，使得能够方便地实现文本纠错\"\"\")\n",
    "\n",
    "# for more details on the example patent and the data source:\n",
    "# paste url into browser to inspect patent at Chinese version of WIPO Patentscope GUI:   \n",
    "# https://patentscope.wipo.int/search/zh/detail.jsf?docId=WO2019085779\n",
    "    \n",
    "# paste url into browser to inspect patent at English version of WIPO Patentscope GUI:\n",
    "# https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2019085779"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence from the Chinese language abstract section of the original patent is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect sentence from summary of patent in original Chinese\n",
    "original_sentence = \"\"\"可以使用从日志中挖掘出的训练语料来训练语言模型，也可以通过对\n",
    "日志中的文本进行分词、统计来整理常用词库.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human translators often produce translations of equivalent quality that nonetheless differ in structure or word choice. BLEU is thus developed to accept single or multiple reference translations by humans. Next, we obtain Chinese-to-English \"reference\" from two human translators via the translation platform Gengo. The translations are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect \"standard\" quality Ch-to-En translations by humans on Gengo platform\n",
    "\n",
    "# inspect Ch-to-En human translation #1 of sentence from patent summary\n",
    "reference_human1_sentence = \"\"\"The training corpus extracted from a log can be used \n",
    "to train the language model, or the common lexicon can be sorted by \n",
    "segmenting and counting text in the log.\"\"\"\n",
    "\n",
    "# inspect Ch-to-En human translation #1 of full patent summary\n",
    "reference_human1_summary = (\"\"\"The invention discloses a machine processing and \\\n",
    "text error correction method and device, a computing device, and a \\\n",
    "storage medium, specifically comprising corrected and rewritten text \\\n",
    "pairs of incorrect  text and corresponding correct text. The corrected \\\n",
    "and rewritten text pairs serving as a training corpus to train the \\\n",
    "machine processing model, thereby preparing a machine processing model \\\n",
    "suitable for text error correction. Through extraction of corrected and \\\n",
    "rewritten text pairs from a log, the machine processing model can be \\\n",
    "trained and thus made fit for text correction by inputting the first text \\\n",
    "into the machine processing model to get the second text, that is the \\\n",
    "error correction result text. In addition, the language model or the \\\n",
    "common lexicon can be used to determine whether the first text needs to \\\n",
    "be corrected. The training corpus extracted from a log can be used to \\\n",
    "train the language model, or the common lexicon can be sorted by \\\n",
    "segmenting and counting text in the log. This is how to easily implement \\\n",
    "text error correction.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect Ch-to-En human translation #2 of sentence from patent summary\n",
    "reference_human2_sentence = \"\"\"It can use the practice language material gathered \n",
    "from the diary or daily journal to train the language model, and it can\n",
    "also initialize the common vocabulary bank through the segmentation and\n",
    "analysis of the diary or daily journal text.\"\"\"\n",
    "\n",
    "# inspect Ch-to-En human translation #2 of full patent summary\n",
    "reference_human2_summary = (\"\"\"This invention makes public a machine processing and\n",
    "text error correction method and hardware, computing equipment and storage \n",
    "medium, and specifically pairs error text with the corresponding corrected \n",
    "and modified correct text. It uses this text pair as training material for \n",
    "the machine processing model, and from there prepares the machine processing\n",
    "model that is applied to the text correction. It can train the machine processing\n",
    "model using a diary or daily journal and make it suitable for text correction.\n",
    "The first text version is inputted into the machine processing model to get \n",
    "the second text version, which is the corrected text. Additionally, it can \n",
    "also use a stored language model or common vocabulary bank to determine if \n",
    "the first text version needs correction. It can use the practice language \n",
    "material gathered from the diary or daily journal to train the language model,\n",
    "and it can also initialize the common vocabulary bank through the segmentation\n",
    "and analysis of the diary or daily journal text. Through all this, text \n",
    "correction is conveniently implemented.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we source \"candidate\" machine translations from two separate machine learning algorithms: Google Translate, and the World Intellectual Property Organization (WIPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect machine translation by Google Translate of sentence from summary\n",
    "candidate_google_sentence = \"\"\"The language model can be trained using the training \n",
    "corpus extracted from the log, or the common lexicon can be organized by \n",
    "segmenting and counting the text in the log.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect machine translation by Google Translate of full summary\n",
    "candidate_google_summary = \"\"\"The invention discloses a machine processing and\n",
    "text error correction method and device, a computing device and a\n",
    "storage medium, and particularly comprises an error correction\n",
    "rewriting pair of an error text and a corresponding correct text, and\n",
    "an error correction rewriting pair as a training corpus, and a machine\n",
    "processing model. Training is performed, thereby preparing a machine\n",
    "processing model suitable for text correction. The machine processing\n",
    "model can be trained to mine the error correction by mining the error\n",
    "correction rewrite pair from the log. The first text is input into the\n",
    "machine processing model to obtain a second text, that is, an error\n",
    "correction result text. In addition, you can use the language model or\n",
    "common lexicon to determine whether the first text needs to be corrected.\n",
    "The language model can be trained using the training corpus extracted\n",
    "from the log, or the common lexicon can be organized by segmenting and\n",
    "counting the text in the log. Thereby, text correction is facilitated.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The invention discloses a machine processing and text error correction method',\n",
       " 'and device, a computing device and a storage medium, and particularly comprises',\n",
       " 'an error correction rewriting pair of an error text and a corresponding correct',\n",
       " 'text, and an error correction rewriting pair as a training corpus, and a',\n",
       " 'machine processing model. Training is performed, thereby preparing a machine',\n",
       " 'processing model suitable for text correction. The machine processing model can',\n",
       " 'be trained to mine the error correction by mining the error correction rewrite',\n",
       " 'pair from the log. The first text is input into the machine processing model to',\n",
       " 'obtain a second text, that is, an error correction result text. In addition,',\n",
       " 'you can use the language model or common lexicon to determine whether the first',\n",
       " 'text needs to be corrected. The language model can be trained using the',\n",
       " 'training corpus extracted from the log, or the common lexicon can be organized',\n",
       " 'by segmenting and counting the text in the log. Thereby, text correction is',\n",
       " 'facilitated.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = textwrap.wrap(candidate_google_summary, width=79)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The invention discloses a machine processing and\\ntext error correction method and device, a computing device and a\\nstorage medium, and particularly comprises an error correction\\nrewriting pair of an error text and a corresponding correct text, and\\nan error correction rewriting pair as a training corpus, and a machine\\nprocessing model. Training is performed, thereby preparing a machine\\nprocessing model suitable for text correction. The machine processing\\nmodel can be trained to mine the error correction by mining the error\\ncorrection rewrite pair from the log. The first text is input into the\\nmachine processing model to obtain a second text, that is, an error\\ncorrection result text. In addition, you can use the language model or\\ncommon lexicon to determine whether the first text needs to be corrected.\\nThe language model can be trained using the training corpus extracted\\nfrom the log, or the common lexicon can be organized by segmenting and\\ncounting the text in the log. Thereby, text correction is facilitated.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_google_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect machine translation by WIPO of full summary\n",
    "candidate_wipo_sentence = (\"\"\"The training corpus extracted from the log can be \n",
    "used to train the language model and also, through text segmentation and \n",
    "statistical analysis of text in the log compile a lexicon of commonly \n",
    "used words.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect machine translation by Google Translate of full summary\n",
    "candidate_wipo_summary = (\"\"\"The present invention discloses a machine processing and text correction method and device, \n",
    "computing equipment and a storage medium. Specifically comprising corrected and rewritten text pairs of incorrect \n",
    "text and corresponding correct text, the corrected and rewritten text pairs serving as a training corpus for training\n",
    "a machine processing model, and in this way developing a machine processing model for use in text correction. \n",
    "Through extraction of corrected and rewritten text pairs from a log, the machine processing model can be trained \n",
    "and thus made fit for text correction by inputting a first text into the machine processing model to obtain a second\n",
    "text i.e. a corrected text result. Moreover, a language model or a lexicon of commonly used words can be used to \n",
    "assess whether text needs correction. The training corpus extracted from the log can be used to train the language \n",
    "model and also, through text segmentation and statistical analysis of text in the log compile a lexicon of commonly \n",
    "used words. Thus, text correction can be made easier and more convenient.\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dictionary):\n",
    "    \"\"\"converts a dictionary of texts to a list of lists of tokens\"\"\"\n",
    "    returned_list = []\n",
    "    for key, value in dictionary.items():\n",
    "        list_val = value.split()\n",
    "        returned_list.append(list_val)\n",
    "    return returned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizes references in a dictionary\n",
    "references_dict = {'reference_sentence_1': human1_sentence\n",
    "                  ,'reference_sentence_2': human2_sentence}\n",
    "\n",
    "candidates_dict = {'candidate_sentence_1': google_sentence\n",
    "                  ,'candidate_sentence_2': wipo_sentence}\n",
    "\n",
    "\n",
    "# tokenizes translations using helper function\n",
    "reference_list = tokenize(references_dict)\n",
    "candidates_list = tokenize(candidates_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of bigrams\n",
    "bi_grams = list(ngrams(candidates_list[0], 1))[0:3]\n",
    "print(f\"Bi-gram examples from Google's translation: {bi_grams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of bigrams\n",
    "bi_grams = list(ngrams(candidates_list[0], 2))[0:3]\n",
    "print(f\"Bi-gram examples from Google's translation: {bi_grams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of bigrams\n",
    "bi_grams = list(ngrams(candidates_list[0], 3))[0:3]\n",
    "print(f\"Bi-gram examples from Google's translation: {bi_grams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a list of tuples containing 4-grams\n",
    "four_grams = list(ngrams(candidates_list[0], 4))[0:3]\n",
    "print(f\"4-gram examples from Google's translation: {four_grams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence-level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'references_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2c15a02406a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BLEU-1 for Google translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbleu1mod_google\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreferences_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu1mod_google\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'references_list' is not defined"
     ]
    }
   ],
   "source": [
    "# BLEU-1 for Google translation\n",
    "bleu1mod_google = modified_precision([references_list[0]], candidates_list[0], n=1)\n",
    "print(float(bleu1mod_google))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU-2 for Google translation\n",
    "bleu2mod = modified_precision([references_list[0]], candidates_list, n=2)\n",
    "print(float(bleu2mod))\n",
    "\n",
    "# BLEU-3 for Google translation\n",
    "bleu3mod = modified_precision([references_list[0]], candidates_list, n=3)\n",
    "print(float(bleu3mod))\n",
    "\n",
    "# BLEU-4 for Google translation\n",
    "bleu4mod = modified_precision([references_list[0]], candidates_list, n=4)\n",
    "print(float(bleu4mod))\n",
    "\n",
    "# BLEU-4 for Google translation, which is default for function\n",
    "bleu4mod = modified_precision(references_list[0]], candidates_list, n=4)\n",
    "print(float(bleu4mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .35 BLEU score for WIPO's translation\n",
    "bleu_wipo = sentence_bleu([i[0]], i[3])\n",
    "bleu_wipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_google_2refs = sentence_bleu([i[0], i[1]], i[2])\n",
    "bleu_google_2refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_wipo_2refs = sentence_bleu([i[0], i[1]], i[3])\n",
    "bleu_wipo_2refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu2mod = modified_precision([i[0]], i[2], n=2)\n",
    "print(bleu2mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu3mod = modified_precision([i[0]], i[2], n=3)\n",
    "bleu3mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu4mod = modified_precision([i[0]], i[2], n=4)\n",
    "bleu4mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_four_grams = list(ngrams(i[0], 4))\n",
    "ref_four_grams, len(four_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google_four_grams = list(ngrams(i[2], 4))\n",
    "google_four_grams, len(google_four_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ref_four_grams) = set(google_four_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(google_four_grams) & set(ref_four_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, bleu_score calculates a BLEU-4,which i a score for the overlap of up to 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"the geometric mean of the test corpus’ modified precision scores times an exponential brevity penalty factor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by default, bleu_score calculates a BLEU-4,which is an the overlap of 4-grams overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts google paragraph to a list of sentences\n",
    "google_sentences = sent_tokenize(google_paragraph)\n",
    "print(google_sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_sentences[0], len(google_sentences), type(google_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human1_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect human translation #2 of the original text\n",
    "# conducted through the company Gengo at a \"standard\" level\n",
    "human2_paragraph = \"\"\"This invention makes public a machine processing and text error correction method and hardware, computing equipment and storage medium, and specifically pairs error text with the corresponding corrected and modified correct text. It uses this text pair as training material for the machine processing model, and from there prepares the machine processing model that is applied to the text correction. It can train the machine processing model using a diary or daily journal and make it suitable for text correction. The first text version is inputted into the machine processing model to get the second text version, which is the corrected text. Additionally, it can also use a stored language model or common vocabulary bank to determine if the first text version needs correction. It can use the practice language material gathered from the diary or daily journal to train the language model, and it can also initialize the common vocabulary bank through the segmentation and analysis of the diary or daily journal text. Through all this, text correction is conveniently implemented.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The invention discloses a machine processing and text error correction method and device, a computing device, and a storage medium, specifically comprising corrected and rewritten text pairs of incorrect  text and corresponding correct text. The corrected and rewritten text pairs serving as a training corpus to train the machine processing model, thereby preparing a machine processing model suitable for text error correction. Through extraction of corrected and rewritten text pairs from a log, the machine processing model can be trained and thus made fit for text correction by inputting the first text into the machine processing model to get the second text, that is the error correction result text. In addition, the language model or the common lexicon can be used to determine whether the first text needs to be corrected. The training corpus extracted from a log can be used to train the language model, or the common lexicon can be sorted by segmenting and counting text in the log. This is how to easily implement text error correction.\n"
     ]
    }
   ],
   "source": [
    "print(human1_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
