{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Multilingual Machine Learning\n",
    "### Exploring BLEU Scores using Translations of Patent Data\n",
    "#### by Lee Mackey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the article [Multilingual Machine Learning](https://medium.com/@glmack) published on Medium.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Does your machine learn in Chinese? I don't speak Mandarin or Cantonese so Google Translate gets all the credit \\- good or bad \\- for translating the preceding sentence into: \"您和您的機器學習中文嗎?\" But how might a researcher quickly evaluate the quality of machine translations? This question encapsulates the basic challenge that gives rise to the BLEU metric. BLEU, which stands for bilingual language understudy, is a default measure of machine translation quality and is also sometimes applied to [cross-lingual](https://arxiv.org/abs/1901.07291) natural language processing (NLP) tasks. The metric is well-established in the machine translation space but some analysts question its application to a wider set of tasks beyond the original purpose for which the algorithm was developed. This article takes an initial dive into the lessons, implementations and limits of BLEU using examples drawn from multilingual patent documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "from multilingual_machines import (split_tokens, \n",
    "                                   clean_punctuation,\n",
    "                                   represent_references)\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "import string\n",
    "import json\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example data from 'patent_examples.txt' file in Github repo\n",
    "with open('patent_examples.txt') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Basics of BLEU\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers at IBM developed the BLEU algorithm in 2002 as an efficient method to evaluate machine translations on the basis of correlations with human translations. The original [paper](https://www.aclweb.org/anthology/P02-1040.pdf) by the developers, Papineni and colleagues, is a good place to start if you’re interested in the founding context and objectives of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"650\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.aclweb.org/anthology/P02-1040.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1085f2390>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# browse original BLEU paper\n",
    "IFrame('https://www.aclweb.org/anthology/P02-1040.pdf', width=650, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU is an adjusted precision measure of the matching word sequences between a “candidate” machine translation and one or multiple “reference” human translations. BLEU counts \"n-grams\", a term for word sequences of length *n*, in a machine translation that match the n-grams in a human translation, divided by the total count of n-grams in the machine translation. The measure is adjusted in that it clips the matches to the maximum count of n-gram occurrences in a human translation and also penalizes machine translations that diverge in word length from the reference translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting BLEU score is as a number between 0 and 1, in which 0 represents zero n-gram matches between candidate and reference texts, and 1 might equal a machine translation that is exactly similar to one of the reference texts. In practice, the measure commonly aggregates matches across multiple word sequence lengths \\- 4-grams (four-word sequences), tri-grams (three-word sequences), bi-grams (two-word sequences), and uni-grams (one-word sequences) \\- via a geometric mean of the respective n-gram scores. The algorithm was designed for comparisons at the level of a corpus of sentences, with n-gram matches calculated at the basic unit of a sentence and then combined into a corpus-level score. In the present article, the term document refers to a corpus of sentences. If you're interested in additional resources to understand the algorithm, you might check out the [video tutorial](https://www.youtube.com/watch?v=DejHQYAGb7Q) at deeplearning.ai that discusses the details of the algorithm or the [tutorial](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/) at machinelearningmastery.com that explores the NLTK implementation.  To explore the use cases of the metric in a tangible way, I next apply BLEU using translations of Chinese-language patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying BLEU to Patent Texts\n",
    "---\n",
    "\n",
    "A growing share of patents in the machine learning space are written and filed in Chinese according to a recent [report](https://www.wipo.int/publications/en/details.jsp?id=4386) by WIPO (World Intellectual Property Organization), the global organization that governs patents. To explore the basics of BLEU in this multilingual space, you can first begin with a domestic Chinese patent of an NLP innovation that the e-commerce company Alibaba extended to patent coverage at a global scale. The title of the Chinese language patent is displayed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire translations of Chinese patent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['机器处理及文本纠错方法和装置、计算设备以及存储介质']\n"
     ]
    }
   ],
   "source": [
    "# inspect title of example patent in Chinese language\n",
    "print(data['original_title_cn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional details surrounding the example patent, WIPO's data query tool provides both [English](https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2019085779) and [Chinese](https://patentscope.wipo.int/search/zh/detail.jsf?docId=WO2019085779) language versions that you can inspect in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"650\"\n",
       "            height=\"300\"\n",
       "            src=\"https://patentscope.wipo.int/search/zh/detail.jsf?docId=WO2019085779\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1086ad080>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect Chinese language version of patent using WIPO GUI\n",
    "IFrame('https://patentscope.wipo.int/search/zh/detail.jsf?docId=WO2019085779',\n",
    "       width=650,\n",
    "       height=300)\n",
    "\n",
    "# exchange url or paste in browser to inspect English version using WIPO GUI\n",
    "# https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2019085779"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human specialists often produce translations of equivalent quality that nonetheless differ in word choice and structure. BLEU therefore accepts single or multiple human translations as the basis for comparison to machine translations. Next, I obtain two Chinese-to-English translations of the Chinese language patent from two different human translators via [Gengo](https://gengo.com/), a web-based platform for human translators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The invention discloses a machine processing and text error correction method and device, a computing device, and a storage medium, specifically comprising corrected and rewritten text pairs of incorrect text and corresponding correct text.', 'The corrected and rewritten text pairs serving as a training corpus to train the machine processing model, thereby preparing a machine processing model suitable for text error correction.', 'Through extraction of corrected and rewritten text pairs from a log, the machine processing model can be trained and thus made fit for text correction by inputting the first text into the machine processing model to get the second text, that is the error correction result text.', 'In addition, the language model or the common lexicon can be used to determine whether the first text needs to be corrected.', 'The training corpus extracted from a log can be used to train the language model, or the common lexicon can be sorted by segmenting and counting text in the log.', 'This is how to easily implement text error correction.']\n"
     ]
    }
   ],
   "source": [
    "# inspect human #1's Ch-to-En translation of patent summary\n",
    "reference_human1_summary = data['reference_human1_summary']\n",
    "print(reference_human1_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This invention makes public a machine processing and text error correction method and hardware, computing equipment and storage medium, and specifically pairs error text with the corresponding corrected and modified correct text.', 'It uses this text pair as training material for the machine processing model, and from there prepares the machine processing model that is applied to the text correction.', 'It can train the machine processing model using a diary or daily journal and make it suitable for text correction.', 'The first text version is inputted into the machine processing model to get the second text version, which is the corrected text.', 'Additionally, it can also use a stored language model or common vocabulary bank to determine if the first text version needs correction.', 'It can use the practice language material gathered from the diary or daily journal to train the language model, and it can also initialize the common vocabulary bank through the segmentation and analysis of the diary or daily journal text.', 'Through all this, text correction is conveniently implemented.']\n"
     ]
    }
   ],
   "source": [
    "# inspect human #2's Ch-to-En translation #2 of patent\n",
    "reference_human2_summary = data['reference_human2_summary']\n",
    "print(reference_human2_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can source \"candidate\" machine translations from two separate machine learning algorithms: Google Translate and the World Intellectual Property Organization (WIPO), which are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The invention discloses a machine processing and text error correction method and device, a computing device and a storage medium, and particularly comprises an error correction rewriting pair of an error text and a corresponding correct text, and an error correction rewriting pair as a training corpus, and a machine processing model.', 'Training is performed, thereby preparing a machine processing model suitable for text correction. The machine processing model can be trained to mine the error correction by mining the error correction rewrite pair from the log.', 'The first text is input into the machine processing model to obtain a second text, that is, an error correction result text.', 'In addition, you can use the language model or common lexicon to determine whether the first text needs to be corrected.', ' The language model can be trained using the training corpus extracted from the log, or the common lexicon can be organized by segmenting and counting the text in the log.', 'Thereby, text correction is facilitated.']\n"
     ]
    }
   ],
   "source": [
    "# inspect Google Translate's Ch-to-En machine translation of patent\n",
    "candidate_google_summary = data['candidate_google_summary']\n",
    "print(candidate_google_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The present invention discloses a machine processing and text correction method and device, computing equipment and a storage medium.', 'Specifically comprising corrected and rewritten text pairs of incorrect text and corresponding correct text, the corrected and rewritten text pairs serving as a training corpus for training a machine processing model, and in this way developing a machine processing model for use in text correction.', 'Through extraction of corrected and rewritten text pairs from a log, the machine processing model can be trained and thus made fit for text correction by inputting a first text into the machine processing model to obtain a second text i.e. a corrected text result.', 'Moreover, a language model or a lexicon of commonly used words can be used to assess whether text needs correction. The training corpus extracted from the log can be used to train the language model and also, through text segmentation and statistical analysis of text in the log compile a lexicon of commonly used words.', 'Thus, text correction can be made easier and more convenient.']\n"
     ]
    }
   ],
   "source": [
    "# inspect WIPO's Ch-to-En machine translation by WIPO of patent\n",
    "candidate_wipo_summary = data['candidate_wipo_summary']\n",
    "print(candidate_wipo_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate BLEU scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple implementations and extensions of BLEU, such as the popular [sacreBLEU](https://github.com/mjpost/sacreBLEU) package, among others. The present example begins by calculating scores using the bleu_score module in the Natural Language Toolkit (NLTK), which is based on the original implementation by Papineni et al. The `corpus_bleu` function accepts reference and candidate translations represented as individual word tokens that you can create through standard NLP pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split candidate and reference translations into tokens\n",
    "ref_human1_summary = split_tokens(reference_human1_summary)\n",
    "ref_human2_summary = split_tokens(reference_human2_summary)\n",
    "can_google_summary = split_tokens(candidate_google_summary)\n",
    "can_wipo_summary = split_tokens(candidate_wipo_summary)\n",
    "\n",
    "# clean punctuation from tokens in candidates and references\n",
    "ref_human1_summary = clean_punctuation(ref_human1_summary)\n",
    "ref_human2_summary = clean_punctuation(ref_human2_summary)\n",
    "can_google_summary = clean_punctuation(can_google_summary)\n",
    "can_wipo_summary = clean_punctuation(can_wipo_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a document with multiple sentences, the original BLEU implementation computes the n-gram matches sentence by sentence, then sums the clipped n-gram counts for all the candidate sentences and, lastly, divides by the number of candidate n-grams in the document. As the NLTK documentation states: the original BLEU metric \"calculates the micro-average precision (i.e. summing the numerators and denominators for each hypothesis-reference(s) pairs before the division).\" By default, `corpus_bleu` calculates the geometric mean of equally-weighted n-gram scores comprising 4-grams, tri-grams, bi-grams and unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-gram tuples: [('The', 'invention'), ('invention', 'discloses'), ('discloses', 'a'), ('a', 'machine'), ('machine', 'processing'), ('processing', 'and'), ('and', 'text'), ('text', 'error'), ('error', 'correction'), ('correction', 'method')] \n",
      "\n",
      "4-grams tuples: [('The', 'invention', 'discloses', 'a'), ('invention', 'discloses', 'a', 'machine'), ('discloses', 'a', 'machine', 'processing'), ('a', 'machine', 'processing', 'and'), ('machine', 'processing', 'and', 'text'), ('processing', 'and', 'text', 'error'), ('and', 'text', 'error', 'correction'), ('text', 'error', 'correction', 'method'), ('error', 'correction', 'method', 'and'), ('correction', 'method', 'and', 'device')]\n"
     ]
    }
   ],
   "source": [
    "# inspect bi-gram examples from string human #1's translation\n",
    "bi_grams = list(ngrams(ref_human1_summary[0], 2))[0:10]\n",
    "\n",
    "# inspect 4-gram examples from string of human #1's translation\n",
    "four_grams = list(ngrams(ref_human1_summary[0], 4))[0:10]\n",
    "\n",
    "print(f\"bi-gram tuples: {bi_grams}\", '\\n')\n",
    "print(f\"4-grams tuples: {four_grams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a document with multiple sentences, the original BLEU implementation computes the n-gram matches sentence by sentence, then sums the clipped n-gram counts for all the candidate sentences and, lastly, divides by the number of candidate n-grams in the document. As the NLTK documentation states: the original BLEU metric \"calculates the micro-average precision (i.e. summing the numerators and denominators for each hypothesis-reference(s) pairs before the division).\"\"Instead of averaging the sentence level BLEU scores (i.e. macro-average precision), the original BLEU metric accounts for the micro-average precision (sums the numerators and denominators for each hypothesis-reference(s) pairs before the division).\"\n",
    "\n",
    "There are multiple implementations and extensions of BLEU, such as the popular sacreBLEU package, among others. The present example begins by calculating scores using the bleu_score module in the Natural Language Toolkit (NLTK), which is based on the original implementation by Papineni et al. The corpus_bleu function accepts reference and candidate translations represented as individual word tokens that you can create through standard NLP pre-processing steps. For a document with multiple sentences, the original BLEU implementation computes the n-gram matches sentence by sentence, then sums the clipped n-gram counts for all the candidate sentences and, lastly, divides by the number of candidate n-grams in the document. As the NLTK documentation states: the original BLEU metric \"calculates the micro-average precision (i.e. summing the numerators and denominators for each hypothesis-reference(s) pairs before the division).\" By default, corpus_bleu calculates the geometric mean of equally-weighted n-gram scores comprising 4-grams, tri-grams, bi-grams and unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the results of the BLEU score calculations for the two machine translation examples? You can compare each sentence in the Google and WIPO translations against all sentences in both reference human translations. The `corpus_bleu` accepts hypothesis sentences as a list of lists of hypothesis sentences and reference sentences as a list of lists of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent references to 6 sentence-length WIPO translation\n",
    "refs_list_6 = represent_references(ref_human1_summary, ref_human2_summary, 6)\n",
    "\n",
    "# represent references to 5 sentence-length WIPO translation\n",
    "refs_list_5 = represent_references(ref_human1_summary, ref_human2_summary, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the Google and WIPO translations are of similar quality in relation to the reference translations from human translators on Gengo. The score of the first candidate translation by Google Translate is 0.53 and the score of WIPO is 0.54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Translate BLEU score: 0.53\n",
      "WIPO BLEU score: 0.54\n"
     ]
    }
   ],
   "source": [
    "# calculate BLEU score of Google translation\n",
    "bleu_google = round(corpus_bleu(refs_list_6, can_google_summary[:]), 2)\n",
    "\n",
    "# calculate BLEU score of WIPO translation\n",
    "bleu_wipo = round(corpus_bleu(refs_list_5, can_wipo_summary[:]), 2)\n",
    "\n",
    "print(f\"Google Translate BLEU score: {bleu_google}\")\n",
    "print(f\"WIPO BLEU score: {bleu_wipo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIPO BLEU score: 0.54\n"
     ]
    }
   ],
   "source": [
    "# calculate BLEU score of WIPO translation\n",
    "bleu_wipo = round(corpus_bleu(refs_list_5, can_wipo_summary[:]), 2)\n",
    "print(f\"WIPO BLEU score: {bleu_wipo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this basic example of the application of BLEU in mind, it is helpful to consider the potential limits to the scope of BLEU's application to machine translation and natural language processing tasks more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of BLEU\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a general consensus among researchers that BLEU is expedient for evaluations of machine translation systems. However, some researchers caution that BLEU may not be appropriate for certain aspects of machine translation or wider NLP tasks. A key critique is that BLEU scores may not necessarily correlate well with significant quality differences in human translations. As Callison-Burch and colleagues argue in a highly-cited [paper](https://www.aclweb.org/anthology/E06-1032/): \"there are instances when an improvement in BLEU is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve BLEU in order to achieve a noticeable improvement in translation quality.\" These authors also suggest that BLEU may not be appropriate for comparing machine translation systems that utilize different techniques, detecting aspects of translation that are not modeled well by Bleu, or evaluating infrequent improvements within a test corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers that conduct a literature review of BLEU argue in a recent [paper](https://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00322) that BLEU–human correlations may vary in relation to the details of the systems being evaluated, the exact corpus texts used, and the exact protocol used for human evaluations. The authors, Reiter and colleagues, suggest that BLEU should be evaluated by real-world human evaluations, such as A/B tests to determine contexts in which BLEU demonstrated a reliable correlation with real-world effectiveness. On the basis of the results of the literature review, these authors argue that researchers should approach BLEU as a diagnostic for machine translation at the system level but not as an evaluation technique to measure the output of a system. While there is a recognition of some of the shortcomings of the measure, there is also no clear replacement. Some researchers, such as a recent [paper](https://arxiv.org/abs/1804.08771), call for more clarity in the reporting of BLEU scores, such as including preprocessing schemes, rendering scores un-comparable and implementing standard conventions for researchers in reporting the details of BLEU scores in a standard manner. The points raised in these papers sound notes in caution in applying and interpreting BLEU scores across different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "This article has outlined initial objectives, examples and limits of BLEU scores. To run the full code that accompanies this article in a jupyter notebook, you can head to the Github [repository](https://github.com/glmack/multilingual_machines) for Measuring Multilingual Machines. As patent documents in the machine learning space continue to become increasingly multilingual, particularly from Chinese language patents, the points raised in this article hopefully help to surface and contextualize considerations when applying this algorithm to your particular use cases. At a more general level, if you're working across languages in your NLP workflows, understanding these details of BLEU will help to select if, when and how to use this metric in your projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "zh-cn"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh-cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
