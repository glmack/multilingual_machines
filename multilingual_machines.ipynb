{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Multilingual Machines\n",
    "#### BLEU Scores and Cross-Lingual Machine Learning\n",
    "Accompanies the Medium article titled \"Measuring Multilingual Machines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Lee Mackey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your machine learn in Chinese? 您和您的機器學習中文嗎? I don’t speak a word of Mandarin or Cantonese so Google Translate gets all the credit — good or bad — for the preceding sentence. But how might a researcher quickly evaluate the quality of machine translations? This question encapsulates the basic challenge that gives rise to the BLEU metric. BLEU, which stands for bilingual language understudy, is the default measure of machine translation quality and is also sometimes applied to cross-lingual natural language processing (NLP) tasks. The metric is well-established in the machine translation space but some analysts also question the application to a wider set of NLP tasks beyond the original purpose for which the algorithm was developed. This article takes an initial dive into these issues by exploring lessons, implementations and limits of BLEU using examples drawn from the multilingual space of global patent documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from nltk.translate.bleu_score import (sentence_bleu, corpus_bleu,\n",
    "                                       modified_precision, SmoothingFunction)\n",
    "from nltk import (bigrams, trigrams, ngrams, sent_tokenize, word_tokenize)\n",
    "import sacrebleu\n",
    "from multilingual_machines import split_tokens, clean_punctuation\n",
    "\n",
    "import string\n",
    "import json\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Jupyter theme\n",
    "! jt -t grade3 -fs 95 -altp -tfs 11 -nfs 115 -cellw 66% -T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example data from 'patent_examples.txt' file in Github repo\n",
    "with open('patent_examples.txt') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers at IBM developed the BLEU algorithm in 2002 as an efficient method to evaluate machine translation tasks that would otherwise require human evaluators. The original paper by the developers, Papineni and colleagues, is a good place to start if you’re interested in the founding context and objectives of the algorithm [1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.aclweb.org/anthology/P02-1040.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x113a9bbd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# browse original BLEU paper\n",
    "IFrame('https://www.aclweb.org/anthology/P02-1040.pdf', width=700, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU is an adjusted precision measure of the overlap of word sequences between a “candidate” machine translation and “reference” human translations. BLEU is a measure of precision in that it counts \"n-grams\", word sequences of length n, of a machine translation that match the n-grams in a human translation and divides by the number of n-grams in the machine translation. The measure is adjusted in the sense that it \"clips\" the n-gram count to the maximum number of n-gram occurrences in a human translation and penalizes machine translations that diverge in word length from the reference translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting BLEU score is as a number between 0 and 1, where 0 represents the complete absence of overlap n-grams between candidate and reference texts, and where 1 might equal a machine translation that is exactly similar to one of the reference texts. In practice, this measure typically combines a range of word sequence lengths, including 4-grams (four-word sequences), tri-grams (three-word sequences), bi-grams (two-word sequences), and uni-grams (one-word sequences), via a weighted average of the log of each n-gram score. While the algorithm is designed for comparisons at the level of a document, BLEU score builds up from n-gram calculations at the basic unit of a sentence, and . If you’re interested in additional resources on calculating BLEU, you might start with a fifteen-minute video at https://www.youtube.com/watch?v=DejHQYAGb7Q by Andrew Ng of DeepLearning.Ai. To make BLEU more tangible, we can turn to practice in applying BLEU to examples of translations of Chinese-language patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying BLEU to Patent Texts\n",
    "A growing share of patents in the machine learning space are originally written and filed in Chinese according to a recent report by WIPO, the global organization governing patents. To explore the basics of BLEU in more tangible manner, we first begin with the international filing of a Chinese language patent by the e-commerce company Alibaba for an invention related to natural language processing. The title of the Chinese language patent is displayed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire original Chinese-language patent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['机器处理及文本纠错方法和装置、计算设备以及存储介质']\n"
     ]
    }
   ],
   "source": [
    "# inspect sentence from summary of patent in original Chinese\n",
    "print(data['original_title_cn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"300\"\n",
       "            src=\"https://patentscope.wipo.int/search/zh/detail.jsf?docId=WO2019085779\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x113aa03d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for more details about the example patent, inspect the Chinese version using the WIPO GUI:\n",
    "IFrame('https://patentscope.wipo.int/search/zh/detail.jsf?docId=WO2019085779',\n",
    "       width=700,\n",
    "       height=300)\n",
    "\n",
    "# exchange url or paste in browser to inspect English version of patent using the WIPO GUI:\n",
    "# https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2019085779"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire \"reference\" translations from two human translators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human translators often produce translations of equivalent quality that nonetheless differ in structure or word choice. BLEU therefore compares accepts single or multiple reference translations. We next obtain Chinese-to-English \"reference\" translations from two different human translators via the platform Gengo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The invention discloses a machine processing and text error correction method and device, a computing device, and a storage medium, specifically comprising corrected and rewritten text pairs of incorrect text and corresponding correct text.', 'The corrected and rewritten text pairs serving as a training corpus to train the machine processing model, thereby preparing a machine processing model suitable for text error correction.', 'Through extraction of corrected and rewritten text pairs from a log, the machine processing model can be trained and thus made fit for text correction by inputting the first text into the machine processing model to get the second text, that is the error correction result text.', 'In addition, the language model or the common lexicon can be used to determine whether the first text needs to be corrected.', 'The training corpus extracted from a log can be used to train the language model, or the common lexicon can be sorted by segmenting and counting text in the log.', 'This is how to easily implement text error correction.']\n"
     ]
    }
   ],
   "source": [
    "# inspect human #1's Ch-to-En translation of patent summary\n",
    "reference_human1_summary = data['reference_human1_summary']\n",
    "print(reference_human1_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This invention makes public a machine processing and text error correction method and hardware, computing equipment and storage medium, and specifically pairs error text with the corresponding corrected and modified correct text.', 'It uses this text pair as training material for the machine processing model, and from there prepares the machine processing model that is applied to the text correction.', 'It can train the machine processing model using a diary or daily journal and make it suitable for text correction.', 'The first text version is inputted into the machine processing model to get the second text version, which is the corrected text.', 'Additionally, it can also use a stored language model or common vocabulary bank to determine if the first text version needs correction.', 'It can use the practice language material gathered from the diary or daily journal to train the language model, and it can also initialize the common vocabulary bank through the segmentation and analysis of the diary or daily journal text.', 'Through all this, text correction is conveniently implemented.']\n"
     ]
    }
   ],
   "source": [
    "# inspect human #2's Ch-to-En translation #2 of patent summary\n",
    "reference_human2_summary = print(data['reference_human2_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The training corpus extracted from a log can be used to train the language model, or the common lexicon can be sorted by segmenting and counting text in the log.']\n"
     ]
    }
   ],
   "source": [
    "# inspect Ch-to-En human translation #1 of sentence from patent summary\n",
    "reference_human1_sentence = data['reference_human1_sentence']\n",
    "print(reference_human1_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It can use the practice language material gathered from the diary or daily journal to train the language model, and it can also initialize the common vocabulary bank through the segmentation and analysis of the diary or daily journal text']\n"
     ]
    }
   ],
   "source": [
    "# inspect Ch-to-En human translation #2 of sentence from patent summary\n",
    "reference_human2_sentence = data['reference_human2_sentence']\n",
    "print(reference_human2_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire \"candidate\" machine translations from Google and WIPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we source \"candidate\" machine translations from two separate machine learning algorithms: Google Translate, and the World Intellectual Property Organization (WIPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The invention discloses a machine processing and text error correction method and device, a computing device and a storage medium, and particularly comprises an error correction rewriting pair of an error text and a corresponding correct text, and an error correction rewriting pair as a training corpus, and a machine processing model.', 'Training is performed, thereby preparing a machine processing model suitable for text correction. The machine processing model can be trained to mine the error correction by mining the error correction rewrite pair from the log.', 'The first text is input into the machine processing model to obtain a second text, that is, an error correction result text.', 'In addition, you can use the language model or common lexicon to determine whether the first text needs to be corrected.', ' The language model can be trained using the training corpus extracted from the log, or the common lexicon can be organized by segmenting and counting the text in the log.', 'Thereby, text correction is facilitated.']\n"
     ]
    }
   ],
   "source": [
    "# inspect machine translation by Google of full summary\n",
    "candidate_google_summary = data['candidate_google_summary']\n",
    "print(candidate_google_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The present invention discloses a machine processing and text correction method and device, computing equipment and a storage medium.', 'Specifically comprising corrected and rewritten text pairs of incorrect text and corresponding correct text, the corrected and rewritten text pairs serving as a training corpus for training a machine processing model, and in this way developing a machine processing model for use in text correction.', 'Through extraction of corrected and rewritten text pairs from a log, the machine processing model can be trained and thus made fit for text correction by inputting a first text into the machine processing model to obtain a second text i.e. a corrected text result.', 'Moreover, a language model or a lexicon of commonly used words can be used to assess whether text needs correction. The training corpus extracted from the log can be used to train the language model and also, through text segmentation and statistical analysis of text in the log compile a lexicon of commonly used words.', 'Thus, text correction can be made easier and more convenient.']\n"
     ]
    }
   ],
   "source": [
    "# inspect machine translation by WIPO of full summary\n",
    "candidate_wipo_summary = data['candidate_wipo_summary']\n",
    "print(candidate_wipo_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The language model can be trained using the training corpus extracted from the log, or the common lexicon can be organized by segmenting and counting the text in the log.']\n"
     ]
    }
   ],
   "source": [
    "# inspect machine translation by Google of sentence from summary\n",
    "candidate_google_sentence = data['candidate_google_sentence']\n",
    "print(candidate_google_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The training corpus extracted from the log can be used to train the language model and also, through text segmentation and statistical analysis of text in the log compile a lexicon of commonly used words.']\n"
     ]
    }
   ],
   "source": [
    "# inspect machine translation by WIPO of full summary\n",
    "candidate_wipo_sentence = data['candidate_wipo_sentence']\n",
    "print(candidate_wipo_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple implementations and extensions of BLEU. We begin by calculating scores using the nltk.translate.bleu_score implementation from the Natural Language Toolkit (NLTK) package, which is primarily based on the original algorithm. We use the corpus_bleu method. After preparing the translation texts via standard NLP pre-processing to represent the texts as word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process candidate and reference translations\n",
    "\n",
    "# split sentences into tokens\n",
    "ref_human1_summary = split_tokens(reference_human1_summary)\n",
    "ref_human2_summary = split_tokens(reference_human1_summary)\n",
    "can_google_summary = split_tokens(candidate_google_summary)\n",
    "can_wipo_summary = split_tokens(candidate_wipo_summary)\n",
    "\n",
    "# clean punctuation from tokens\n",
    "ref_human1_summary = clean_punctuation(ref_human1_summary)\n",
    "ref_human2_summary = clean_punctuation(ref_human2_summary)\n",
    "can_google_summary = clean_punctuation(can_google_summary)\n",
    "can_wipo_summary = clean_punctuation(can_wipo_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the corpus_bleu function the two reference sentences as a list of lists of tokens. The algorithm computes the n-gram matches for each candidate sentence and add the clipped n-gram counts for all the candidate sentences. Next, BLEU divides the number of candidate n-grams in the candidate corpus to compute a modified BLEU score for the candidate corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi-grams: [('The', 'invention'), ('invention', 'discloses'), ('discloses', 'a'), ('a', 'machine'), ('machine', 'processing')]\n",
      "four-grams: [('The', 'invention', 'discloses', 'a'), ('invention', 'discloses', 'a', 'machine'), ('discloses', 'a', 'machine', 'processing'), ('a', 'machine', 'processing', 'and'), ('machine', 'processing', 'and', 'text')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect examples of n-grams\n",
    "\n",
    "# return list of bi-grams\n",
    "bi_grams = list(ngrams(ref_human1_summary[0], 2))[0:5]\n",
    "\n",
    "# returns list of four-grams\n",
    "four_grams = list(ngrams(ref_human1_summary[0], 4))[0:5]\n",
    "\n",
    "print(f\"bi-grams: {bi_grams}\")\n",
    "print(f\"four-grams: {four_grams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The number of hypotheses and their reference(s) should be the same ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f361ba8aa3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BLEU-4 example using first sentence in summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbleu4_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_human1_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_human1_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu4_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/default-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     assert len(list_of_references) == len(hypotheses), (\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;34m\"The number of hypotheses and their reference(s) should be the \"\u001b[0m \u001b[0;34m\"same \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The number of hypotheses and their reference(s) should be the same "
     ]
    }
   ],
   "source": [
    "# BLEU-4 example using first sentence in summary\n",
    "bleu4_example = corpus_bleu([ref_human1_sentence], ref_human1_sentence)\n",
    "print(float(bleu4_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4370614964591188\n"
     ]
    }
   ],
   "source": [
    "# BLEU-4 for Google translation with one reference using corpus_bleu\n",
    "bleu4_google_corpusbleu_human1 = corpus_bleu([ref_human1_sentence],\n",
    "                                             can_google_sentence)\n",
    "print(float(bleu4_google_corpusbleu_human1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.010025055942425e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/default-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/anaconda3/envs/default-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# BLEU-4 for Google translation with second reference using corpus_bleu\n",
    "bleu4_google_corpusbleu_human2 = corpus_bleu([ref_human2_sentence],\n",
    "                                             can_google_sentence)\n",
    "print(float(bleu4_google_corpusbleu_human2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4523563820810908\n"
     ]
    }
   ],
   "source": [
    "# BLEU-4 for Google translation with two references using corpus_bleu\n",
    "bleu4_google_corpusbleusm_2refs = corpus_bleu(\n",
    "    [[ref_human1_sentence[0], ref_human2_sentence[0]]], can_google_sentence)\n",
    "print(float(bleu4_google_corpusbleusm_2refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4523563820810908\n"
     ]
    }
   ],
   "source": [
    "# Note: BLEU was not originally intended for sentence-level calculation\n",
    "# NLTK has a sentence_bleu score but this will not return accurate\n",
    "# results across a corpus of sentences (docs)\n",
    "\n",
    "# BLEU-4 for Google translation with two references using sentence_bleu\n",
    "bleu4_google_sentencebleu = sentence_bleu([ref_human1_sentence[0],\n",
    "                                           ref_human2_sentence[0]],\n",
    "                                           can_google_sentence[0])\n",
    "print(float(bleu4_google_sentencebleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48435192067731586\n"
     ]
    }
   ],
   "source": [
    "# BLEU-4 for WIPO translation with two references using corpus_bleu\n",
    "bleu4_wipo_corpusbleusm_2refs = corpus_bleu(\n",
    "    [[ref_human1_sentence[0], ref_human2_sentence[0]]], can_wipo_sentence)\n",
    "print(float(bleu4_wipo_corpusbleusm_2refs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the result? The score of the first candidate translation by Google Translate is 0.45. The score of the second machine translation by WIPO is 0.48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we conduct the same translation on the entire abstract text, the score of Google is .XXX, and the score of WIPO is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original BLEU metric \"calculates the micro-average precision (i.e. summing the numerators and denominators for each hypothesis-reference(s) pairs before the division).\"\"Instead of averaging the sentence level BLEU scores (i.e. macro-average precision), the original BLEU metric accounts for the micro-average precision (i.e. summing the numerators and denominators for each hypothesis-reference(s) pairs before the division).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU-4 for Google translation of summary with two references using corpus_bleu\n",
    "# bleu4_google_summary = corpus_bleu(ref_human1_summary, ref_human2_summary, can_google_summary)\n",
    "# print(float(bleu4_google_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.135281163847754e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/default-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# BLEU-4 for Google translation of summary with two references using corpus_bleu\n",
    "bleu4_google_summary = corpus_bleu(ref_human1_summary, ref_human1_summary)\n",
    "print(float(bleu4_google_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU-4 for WIPO translation with two references using corpus_bleu\n",
    "# bleu4_wipo_summary_corpusbleusm_2refs = corpus_bleu([[ref_human1_sentence[0],\n",
    "#                                                       ref_human2_sentence[0]]],\n",
    "#                                         can_wipo_summary)\n",
    "# print(float(bleu4_wipo_summary_corpusbleusm_2refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: by default, bleu_score calculates a BLEU-4,which is a score for the overlap of up to 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: \"the geometric mean of the test corpus’ modified precision scores times an exponential brevity penalty factor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this basic example of the application of BLEU in mind, we can now discuss some of the potential limits of the application of BLEU to machine translation and natural language processing tasks more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of BLEU in cross-lingual machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be agreement that BLEU is appropriate, even if expedient, for diagnostic evaluation of machine translation systems but some researchers suggest that BLEU may not be appropriate for certain types of translation and NLP tasks. One critique raised by researchers is that BLEU's flexibility with respect to allowed variation in translations may result in documents of differing quality that nonetheless score the same on the BLEU metric [4]. The authors argue \"that there are instances when an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality.\" [3] BLEU may also not be appropriate for comparisons of machine learning systems that employ significantly different strategies. Callison-Burch argue that inappropriate uses for Bleu include: 1) comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches). 2) trying to detect improvements for aspects of translation that are not modeled well by Bleu, and 3) monitoring improvements that occur infrequently within a test corpus. Some researchers suggest that BLEU is not appropriate for wider tasks, such as for evaluation of individual texts, or for scientific hypothesis testing [6]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some researchers raise questions about the construct validity of BLEU as a measure. When conducting a structured literature review, some researchers argue that BLEU–human correlations, and suggest that whether BLEU correlates with human evaluations is very dependent on the details of the systems being evaluated, the exact corpus texts used, and the exact protocol used for human evaluations. (Reiter). As a surrogate endpoint, Reiter argues that BLEU is useful only if such scores \"reliably predict an outcome that is of real-world importance or is the core of a scientific hypothesis we wish to test.\" Reiter finds that none of the \"surveyed papers used real-world human evaluations; that is, they all used human evaluations performed in an artificial context (usually by paid individuals, crowdsourced workers, or the researchers themselves), rather than looking at the impact of systems on real-world users.\" Given these absence of validated findings on BLEU, Reiter calls for A/B tests of correlations between their A/B tests and BLEU (?). \"the results of real-world A/B testing could be used to determine contexts in which BLEU reliably had good correlation with real-world effectiveness.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reiter and others argue that researchers should approaches BLEU as a diagnostic for machine translation at the system level, but not as an evaluation technique to measure the output of a system. Reiter finds that \"the evidence does not support using BLEU to evaluate other types of NLP systems (outside of [machine translation), and it does not support using BLEU to evaluate individual texts rather than NLP systems.\" Reiter argues that BLEU should not be the primary evaluation technique of NLP papers. Reiter argues that this is because of concerns about the validity and reliability of BLEU. \n",
    "While there is a recognition of some of the shortcomings of the measure, there is also no clear replacement. Some researchers call for more clarity in the reporting of BLEU scores [2]. This includes the argument that BLEU is under-specified and contains parameters, that preprocessing schemes have a large effect on scores, rendering scores un-comparable, and because there are not standard conventions for researchers in reporting the details of BLEU scores in a standard manner.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "If you're working across languages in your natural language processing flows, understanding these details of BLEU will help to select when and how to use this metric in your projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
